% Generated by roxygen2 (4.0.2): do not edit by hand
\name{CSSR}
\alias{CSSR}
\title{Use CSSR algorithm to infer Causal State Model}
\usage{
CSSR(data, L, alpha = 0.05, MARGIN = 1, verbose = FALSE)
}
\arguments{
\item{data}{a vector, matrix or list with data.}

\item{L}{length of longest history examined.}

\item{alpha}{a significance level used to decide whether two distributions differ.}

\item{MARGIN}{an argument only relevant if "data" is matrix, needed for apply function. Defaults to 1(rows are time-series sequences).}

\item{verbose}{flag to print out intermediate information. Defaults to FALSE.}
}
\description{
Infers Causal State Model from data
}
\details{
Accepted data at the moment is limited to integer, character or coered to integer numeric vectors,
matrixes and lists with same-class vectors.
Below is the abstract from help from authors of algorithm.
See http://bactra.org/CSSR/ for the source.

CSSR tries to infer the minimal Markovian model capable of generating a
time-series, or set of time-series from the same source.  The program
implements the algorithm proposed in the paper "Blind Construction of Optimal
Nonlinear Recursive Predictors for Discrete Sequences", hereafter BC. [...]
We won't describe the algorithm in any detail here (see BC for that), but the
next two paragraphs say a little about what it produces and how it does it.

The output of the algorithm is a set of states which form a Markov chain.  Each
state has a certain probability of emitting any of the symbols in the original
time series.  The current state and the symbol it emits fix the next state.
(The states are "future-resolving", if you're from nonlinear dynamics, or
"deterministic", if you're from automata theory.)  Each state, moreover,
corresponds to a distinct set of strings, in the following sense.  If the state
A contains a string w, and at time t the time-series ends with w, then at time
t the Markov chain is in state A.  The set of states, their transition
probabilities and connections, is called the state machine.

The algorithm uses a recursive inference procedure to find the simplest set of
states with the above properties that can reproduce the statistical properties
of the data.  If we could give the algorithm an infinitely long time series,
and let it consider infinitely long sub-strings, it would produce the causal
states of the process, which are its ideal predictors (see BC for a formal
definition).  Since we have only finite data, there is always some probability
that the inferred or estimated states are not the true causal states.
Nonetheless, for the rest of this file, when we say "causal states", we mean
the estimated causal states.

Some Suggestions About Parameters

It is always good to use as much data as you can.  While it is generally good
practice to hold back some data for testing or cross-validation, we recommend
that this be minimized.  High-entropy processes are especially data-hungry.
(See BC.)  For reference, let us call the number of data-points N.

The two key parameters of the program are the maximum history length, L, and
the significance level used in the test, s. For any given process, there is a
minimum history length M, such that the true states cannot be found if L < M.
The number of states returned may be less than the correct number _or higher.
If L >= M, and there is enough data, there will generally be a "plateau" of
values of L where the correct number of states is returned.  For fixed N, if we
keep increasing L, then past a certain point there are not enough examples of
each string in the data.  This tends to erroneously create new states, which
spawn others through determinization.  Thus there is generally a "blow-up" when
L is too large (relative to N and s).  A rough guide-line is to limit L to no
more than log(N)/log(k), where k is the alphabet size (see BC for
                                                       details).

In general, one should use as small an L as possible, since under-sampling,
even before the blow-up, will reduce the accuracy of many probability
estimates.  Blow-up can be delayed by reducing s --- that is, reducing the
probability of mistakenly splitting a state --- but this carries the risk of
failing to create valid new states.  We suggest exploring the data at low L and
high s initially, and then increasing L and lowering s.  If a stable
architecture is found, it should be recorded at the lowest possible L.
}
\examples{
even <- csm(data.frame(c("A", "A", "B"),c("A", "B", "A"), c(0, 1, 1), c(0.5, 0.5, 1)))
data <- simulate(even, 1000)$symbols
acsm <- CSSR(data, 3, 0.001)
}

